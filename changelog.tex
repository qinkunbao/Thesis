\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Changelog}
\author{Qinkun Bao}
\date{May 2021}

\begin{document}

\maketitle
Thank you for reviewing my dissertation. I have gone through the details of each comment and revised my dissertation multiple rounds to eliminate any logic flows and fix all the typos and grammar errors. In summary, the revisions cover the following aspects.


\begin{enumerate}
\item \textbf{Fuzzing – Why did you use fuzzing?}

It is just my personal choice.

Abacus identifies and quantifies side-channel leakages in a single trace. So it suffers from the problem of code coverage.  That is, Abacus misses side-channel leakages in the unexecuted code path. 

I have a few choices to address the limitation. First, I can use static analysis (model checking or static program analysis). Although I am interested in static analysis, I am not a big fan of current static analysis tools, especially for side-channel research. There are several papers on static side-channel analysis. Nearly all of these tools are used to prove the absence of side-channel leakages. However, most of them have high false-positive rates. 

On the other hand, there are many success stories with dynamic tools (e.g., SAGE, AFL, Valgrind) in recent years. In particular, I find fuzzing is becoming a hot trend these years, especially after DARPA CGC in 2016.

\item \textbf{Discuss the limitations of your research
e.g., can only handle address based …
you might want to include some discussion on the points (3), (4), and (5) below.}

Like many other research projects, the research presented in the dissertation has a few limitations.

a. It is true that the methods in the dissertation can not find all kinds of side-channel leakages. No tool can find all forms of leakages. Even for address-based side-channel leakages, Abacus and Quincunx can not find all the leakages due to the code coverage problem. 

b. The quantification result only applies to specific threat models. For example, Abacus quantifies the number of leaked bits during one real execution. So it is possible that one leakage site leaks little information in one execution but leaks much information in another execution.

c. It is true that the research in the dissertation can only handle address-based side-channel attacks. Many side-channel attacks infer secret data based on other side-channel signals (e.g, timing, EM signals). However, the root cause of many of those attacks is still the same. That is, the program accesses different addresses when it processes different input secrets.

\item \textbf{Can you extend your research (especially the quantification part) to include other side channel signals?}

It is possible to extend the quantification part to include other side-channel signals. The key is to model the noises (e.g., Sound, CPU usages, EM signals) properly. In my dissertation, we assume that an attacker can have a noise-free observation. While a noise-free observation is possible for some address-based side-channel attacks, the assumption is too ideal for many other side-channel attacks. The good news is that there are some mature methods in information theory to model the noises (e.g., Gaussian noise). The noisy-channel coding theorem provides a computable method to estimate the information can be reliably transferred between the source (confidential data) and the destination (attackers). From a research project aspect, I hope to have a stronger motivation before starting the project. 
\item \textbf{A bits vs. Q bits. Lower bounds?
       Improve the presentation
       Make the comparison more meaningful
       Etc.}
       
       
To address the above concerns, I have made the following revisions.

a. I revised the description in Chapter 5. I abandoned the term lower bound.

d. I revised Table 5.3. The evaluation section does not compare Quincunx with Abacus in the current version because they have different threat models.
\item \textbf{iid assumption – does it fit your research on side-channel?}

It is true that Quincunx requires the random input variables to be independent and identically distributed. That's why I applied a blackbox fuzzing instead of a coverage-guided fuzzing. As long as each time we get a random input from the whole input space (Populations) that is independent of the previous input, the input variables should satisfy the i.i.d. assumption. For AES-128 encryption, each time we randomly generate a 128 bit key from $0$ to $2^{128}$, the sequence of key values follows the i.i.d. assumption.

The situation is a little different for a deep learning model. In the dissertation, we evaluated Quincunx on a deep learning model. Each time, we randomly select an item from a dataset (Samples). If the dataset is biased, then we can not get the correct results. The good news is that the most well-known data in machine learning is i.i.d. i.i.d. is the foundation of supervised machine learning algorithms. In my dissertation, we used MINIST, a well-known handwriting dataset. We believe the dataset is i.i.d.

I have revised my dissertation to include the discussion on iid.


\item \textbf{Some feedback I gave before your defense.}

As suggested by Dr. Wu, I added a new chapter on Discussion and Limitations, and discussed potential ways and related work to the mitigated reported side.

\item \textbf{Improve Grammar.}

Dr. Larus helped me fix many grammar issues. I have revised the dissertation multiple times. In addition, I used Grammarly to proofread my dissertation.
\end{enumerate}

I have incorporated some discussions into my dissertation. Please let me know if you have any feedback.

\end{document}
