\chapter{Discussion and Limitations}\label{chapter6}
\section{Fast and Precise Side-channel Vulnerability Detection}
\subsection*{Code Coverage} \detect{} analyzes one native x86 execution trace each time. The design, which is very precise in terms of true leakages compared to many static source code methods~\cite{197207,BacelarAlmeida:2013:FVS:2483313.2483334}, share common limitations of dynamic approaches. \detect{} may only cover part of the code. Each time we only get one single execution trace. Therefore, we may neglect some side-channel vulnerabilities not covered by the traces analyzed. However, this is not a crucial problem for analyzing cryptographic libraries, because cryptographic libraries are designed to have the same code coverage under various inputs. The evaluation result also confirms the above intuition. For symmetric encryption, there is no secret-dependent control-flow transfers during our evaluation. RSA implementations have several secret-dependent control-flow transfers. Many of them are bound checks, which do not leak much information and have negligible effects on the whole code coverage as well.

\subsection*{Design Choices} At the early stage of the project, we use SMT solvers to get the solutions of constraints. However, we find for some ciphers (RSA), \detect{} times out after several days. The profiling result shows that the tool spends most its time in Z3. Later, we use a sampling method to detect side-channel leakages. The sampling method seems simple and may miss some leakages in theory. However, the evaluation result shows \detect{} can identify all leakages found by the previous work~\cite{203878,236338,Brotzman19Casym}.  

\subsection*{Cache Model} We use a simple cache model in this work. Recent works such as CacheAudit and CaSym~\cite{Brotzman19Casym,182946} use an LRU cache. First, there is no fundamental difficulties for us to adopt a more realistic cache models. Second, the LRU model is still a substantial simplification of a real cache. However, the choice of a logical model of cache can bring expensive overheads. For example, CaSym can only analyze small code snippets.

\subsection*{Leakages} While recent works have reported lots of potential side-channel vulnerabilities, most of them are not patched by developers. The DES implementation of OpenSSL has a few side-channel leakages, but given the end life status of DES, it is still unpatched for the worth of engineering effort. \detect{} also detects several leakages in asymmetric ciphers such as RSA. After we manually analyze these leakage sites, we find many of them leak very little or useless information (e.g., the length of big numbers), which may partially explain why developers are not interested in fix all the side-channel leakages. 

\section{Precise Analysis on Single-trace Attacks}
While recent work found many side-channel vulnerabilities,
we note that many of them are been patched by developers.
Side-channels are ubiquitous in software, and it is difficult to fix all of them.
We need a tool that estimates the sensitivity of each vulnerability
so software engineers can focus on
``severe'' leakages. For example, \tool{} reports that
the modular exponentiation using square and multiply algorithms can
leak more information than a key validation function.

Software developers can use \tool{} to find severe vulnerabilities
and reason about countermeasures.
\tool{} estimates the amount of leaked information for each side-channel leakage
in one execution trace. \tool{} is useful for software
engineers to test programs and fix vulnerabilities.
The design, which is more precise in reporting true leakages as compared with other static
methods~\cite{197207,BacelarAlmeida:2013:FVS:2483313.2483334}, obviously misses leakages on unexplored traces. The amount of leaked information quantified by \tool{} also depends on the value of secret keys.
However, as the tool is intended for debugging and testing,
we think it is a software engineer's responsibility to select the input key and trigger
the path in which they are interested. It is not a problem for crypto software 
since virtually all keys follow similar computational paths.

We use the amount of leaked information to represent the sensitivity level of
each side-channel vulnerability. Although imperfect, \tool{} produces a reasonable
measurement for each leak. For example, the simple modular exponentiation is
notoriously famous for multiple side-channel attacks~\cite{kocher1996timing}.
During the execution, a single leak point may execute multiple times
and each time leak a different bit. In this case, \tool{} reports that the
vulnerability can leak the whole key. However, not every leak point inside a
loop is severe. If a site in the loop leaks the same bit from the
original key, and these leaks are not independent. \tool{} captures most
fine-grained information by modeling each leak during the execution as a
formula and the conjunction of the formulas to describe its total effect.
Some leakage sites (e.g., square and multiply)
can leak one particular bit of the original key, but some leakage sites leak one bit
from several bytes in the original key. \tool{} can capture the dependency among the leaks and
reports more precise leakage information.

\tool{} reaches full precision if the number of estimated leaked bits
equals to Definition~\ref{chapter4:def}.
\tool{} may lose precision from the
memory model it uses in theory. However, we did not find false positives
caused by the imprecise memory model during our evaluation.
Sampling introduces imprecision but with a probabilistic guarantee.
However, during the evaluation, we find that \tool{} cannot estimate
the amount of leakage for some leakage sites in a reasonable time,
which means the number of $K^o$ is very small. According to Definition~\ref{chapter4:def}, it means the leakage is very severe. 
\section{Precise Analysis on Multiple-trace Attacks}
This paper presents an approach that tries to give a conservative
estimate of the amount of leaked information by address-based
side-channel attacks. These attacks exploit the data-flow from secrets
to load address and the data-flow from the data-flow from secrets to
branch conditions to retrieve secrets based on the observation on the
memory accesses. Although these kinds of side-channel leakages have
been discovered for decades, the up-to-date software still has some
side-channel leakages. For some of these vulnerabilities, developers
do not fix them because they think these side-channel vulnerabilities
are not important. To show the importance of these side-channel
leakages, one way is to demonstrate an end-to-end attack based on the
vulnerability. However, demonstrating an end-to-end attack often need
a lot of manual effort and the domain knowledge of the victim program,
which is not often the case in practice. It is good to have a tool
that automatically assesses the severity level of these side-channel
leakages. So it would be better to have a proper metric to quantify
the side-channel leakage. However, we find previous side-channel
quantification tools are developed to ensure the noninterference of
the program. They use the over-approximation heuristics method to
quantify the leakage. For example, CacheAudit estimates that a 128-bit
AES encryption can leak more than 128 bits. As a result, even these
tools report severe leakage, it does not mean the program has a truly
severe leakage.

Our tool can give a conservative estimation of the
side-channel leakage based on Channel Capacity. The channel capacity
measures the information flow between the source and the
destination. One useful characteristic of channel capacity is not
affected by the input, which is useful because we cannot
assume the input secrets of the software in practice. In the paper, we
use the sampling result to approximate the true value of the
channel capacity. We prove the sampling method presented in the paper can only give the conservative estimation of the amount of the true leakage.

\ctool{} requires the random input variables to be independent and identically distributed (i.i.d.). That's why I apply a black box fuzzing instead of a coverage-guided fuzzing. As long as each time we get a random input from the whole input space (Population) that is independent of the previous input, the input variables should satisfy the i.i.d. assumption. For AES-128 encryption, each time we randomly generate a 128 bit key from $0$ to $2^{128}$, the sequence of key values follows the i.i.d. assumption. The situation is a little different for machine learning libraries. In the dissertation, we evaluated \ctool{} on a deep learning model. Each time, we randomly select an item from a dataset (Sample). If the dataset is biased, then we can not get correct results. The good news is that the most well-known data set in machine learning is i.i.d.. i.i.d. is the foundation of supervised machine learning algorithms. In my dissertation, we used MINIST, a well-known handwriting dataset. The dataset is widely used in many supervised machine learning tasks.  We believe the i.i.d. assumption holds under the circumstance.

\ctool{} is a dynamic approach. So it bears the same limitations of dynamic approaches as well. \ctool{} may have the coverage problem and can miss some side-channel vulnerabilities. It is usually not a crucial problem for cryptography libraries as cryptography libraries are designed to have the same control flow under various inputs. For other libraries such as graphic rendering, machine learning, \ctool{} is very likely to miss some side-channel vulnerabilities. However, \ctool{} is not designed to find side-channel vulnerabilities. The goal of \ctool{} is to pick up really serious side-channel vulnerabilities from the numerous vulnerabilities. So we do not think it is the main limitation of \ctool{}. However, users of \ctool{} should be aware of the code coverage problem. 

