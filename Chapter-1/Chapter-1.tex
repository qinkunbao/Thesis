% !TEX root = ../YourName-Dissertation.tex

\chapter{Introduction} \label{chapter1:introduction}

Side channels are inevitable in modern computer systems as the sensitive
information may be leaked through many kinds of inadvertent behaviors, such as power,
electromagnetic radiation, and even
sound~\cite{agrawal2002side,kar20178,chari1999towards,217605,genkin2014rsa}.
Among them, software-based side channel attacks, such as cache attacks, memory page
attacks, and controlled-channel attacks, are especially common and have been
studied for
years~\cite{7163052,217543,217589,lee2017inferring,191010,liu2015last}. These
attacks result from vulnerable software and shared hardware components.
By observing program outputs or hardware behaviors, attackers can infer program
execution flows that manipulate secrets and guess secrets such as encryption
keys~\cite{Osvik2006,Gullasch:2011:CGB:2006077.2006784,203878,10.1007/978-3-540-45238-6_6}.


\section{Side-channel}

\subsection{Attack}

\subsection{Defense}

Regarding the root cause of software-based side channel attacks, many of them originate
from two specific circumstances: data flow from secrets to load
addresses and data flow from secrets to branch conditions. We refer to them as
 secret-dependent memory-access and control-flow, respectively. A
central problem is eliminating these two code patterns. 
Recent
works~\cite{203878,217537,Wichelmann:2018:MFF:3274694.3274741,Brotzman19Casym,236338,182946},
find plenty of side-channel vulnerabilities. For example,
DATA~\cite{217537} reports 2,246 potential leakage site for the RSA
implementation in OpenSSL\@. 
%After some inspections, 1,510 leakages are dismissed. But it
%still leaves 460 data-access leakages and 278 control-flow leakages. 
However, we find most of the reported vulnerabilities are not fixed because
of the following reasons.
First, many side-channels vulnerable implementations have better performance.
Those vulnerabilities are well-known for years. For example,
Symmetric encryptions like AES and DES still uses lookup tables (T-tables), which
is fast but notoriously known to be vulnerable to side channels.
As for asymmetric encryptions, most implementations of RSA, adopt the CRT optimization,
which is faster but vulnerable to fault attacks~\cite{aumuller2002fault}.
Second, side-channels are numerous and it is hard to fix all these vulnerabilities,
let alone the majority of them are negligible. 
That is, some vulnerabilities can result in the key being 
entirely compromised~\cite{184415, aumuller2002fault}, but many other vulnerabilities prove to be less
severe in reality. Therefore, we need a proper quantification metric to 
assess the sensitive level of side-channel vulnerabilities.

Previous attempts like static methods~\cite{182946,5207642}, usually with
abstract interpretations, can give a leakage upper bound, which is useful to
justify the implementation is secure if they report zero or little leakage.
However, they cannot indicate how severe the leakage is because of the
over-approximation method they apply. For example, CacheAudit~\cite{182946} reports that the upper
bound leakage of AES-128 exceeds the original key size! The dynamic methods take
another approach with a concrete input and run the program in a real
environment. Although they are very precise in terms of actual leakages, no
existing tool can precisely assess the severity of the vulnerabilities in production
software. 

To overcome these limitations, we propose a novel method to quantify information
leakages precisely. Unlike previous works that only consider the
``average'' information leakage, we study the problem based on real attack
scenarios. The average information leakage model assumes the target program has
\emph{variable} or \emph{random} sensitive information as inputs when an attack is
launched. However, for real-world attacks, an adversary may run the target
problem with the \emph{fixed} unknown sensitive information
as the input. Therefore, the previous threat model cannot model real attack
scenarios. In contrast, our method is more precise and fine-grained. 
We quantify
the amount of leaked information as the cardinality of the set of possible
inputs based on attackers' observations.
Before an attack, an adversary has a large but finite input space. Every time
when the adversary observes a leakage site, he can eliminate some potential
inputs and reduce the size of the input space. The smaller the input space is,
the more information is obtained. In an extreme case, if the size of the
input space reduces to one, the adversary can uniquely determine the input, 
which means all the secret information (e.g., the whole secret key) is
leaked. By counting the number of distinct inputs, we can quantify the
information leakage precisely.

We use constraints generated from symbolic execution to model the relation 
between the original sensitive input
and the attacker's observations. Symbolic execution can
provide fine-grained information but is usually believed to be an expensive
operation in terms of performance. Therefore, existing dynamic symbolic
execution works~\cite{203878,236338,Brotzman19Casym} either only analyze
small programs or apply some domain knowledge~\cite{203878} to simplify the analysis. We
systematically exam the bottleneck of the trace-oriented symbolic execution and optimize it
to be scalable to real-world cryptosystems.

We apply the above technique and build a tool called \tool{},
%\footnote{CleverHans is a horse that can ``count''. Our tool uses an advanced
%%method to count the number of leaked bits from side channels.} 
to discover potential information leakage sites as well as estimate how
many bits they can leak for each leakage site. 
First, we collect dynamic execution traces for each target
libraries and then run symbolic execution on each instruction. In this way, we model
each side-channel leakage as a logic formula. The sensitive input is divided into
several independent bytes, and each byte is regarded as a unique and free symbol. Those
formulas can precisely model side-channel vulnerabilities. Then we use the conjunction
of those formulas to model the same leaks in the source code but appears in the different location of
the execution trace file (e.g., leakages inside a loop).
Finally, we introduce Monte Carlo
sampling method to estimate the single and combined information leakage. 

We apply \tool{} on both symmetric and asymmetric ciphers from real-world crypto
libraries, including OpenSSL, mbed TLS and Monocypher\@. The experimental result confirms
that \tool{} can precisely identify previously known vulnerabilities, report
how much information is leaked and which byte in the original sensitive buffer
is leaked. We also test \tool{} on side-channel free algorithms. \tool{} has no
false positives.
Although some of the analyzed crypto libraries have a number of
side-channels, they actually leak very little information. Also, we also show the
of widely deployed software countermeasures can mitigate side channels.
\tool\ also discovers new vulnerabilities. With the help of \tool{}, we confirm
that those vulnerabilities are severe.

In summary, we make the following contributions:

\begin{itemize}
      \item We propose a novel method that can quantify fine-grained leaked
            information from side-channel vulnerabilities to match real attack
            scenarios. Our method is different from previous ones in that we
            model real attack scenarios more precisely, while the previous
            research only models the ``average'' or ``random'' case. 
            We transfer the information quantification problem into a counting
            problem and use the Monte Carlo sampling method to estimate the
            information leakage.
            % compared to previous results and
            %%   We model each side-channel vulnerabilities as math formulas %
            %and mutiple side-channel vulnerabilities can be seen as the %
            %conjunction of those formulas, which precisely models the % program
            %semantics.

      %%\item  Some initial results indicate the sampling
      %%     method suffers from the curse of dimensionality problem. Therefore, we
      %%      design a guided sampling method and provide the
      %%      corresponding error estimate.

      \item We implement the proposed method into a practical tool and apply it
            on several real-world software. \tool{} successfully identifies
            memory-related side-channel vulnerabilities and calculates the
            corresponding information leakage. 
            Our results are surprisingly different, much more useful in practice.
            The information leakage results
            provide detailed information that can help developers to fix the
            reported vulnerabilities.
\end{itemize}

\section{Binary Analysis}
Binary analysis is the process of automatically inferring program behaviors from
the binary executable.
It plays a critical role from a security viewpoint. For many programs, like malware, Commercial off-the-shelf (COTS) application, source code are typically not
available. Besides, some low-level information, including memory accesses and cache
accesses, is only available on the binary code level. As many side-channel vulnerabilities
are typically low-level problem, we believe binary analysis are more suitable than
source code level analysis.

Binary analysis can be categorized into two groups depending when the analysis is
launched.
\begin{itemize}
    \item Static Analysis infer analyzing the binary program without running it. Previous studies focus on addressing several open problems, including identifying
    function entry points~\cite{184521, Wang17a}, resolving indirect jumps, disambiguating code and data and solving alias memory address. Static analysis
    lacks the run-time information, which may lead to false positives and false 
    negatives. However, static analysis typically can have better coverage as it can
    reason about every possible execution paths.
    \item Dynamic Analysis involving analyzing a binary code as it executes. Tools
    that perform dynamic analysis usually need to instrument the binary code.  
    Although dynamic analysis can only reason only one execution path. It is usually more precise than static analysis.
\end{itemize}

In this dissertation, we detected side-channel vulnerabilities by trace-based analysis.
It consists of two steps. The first step, called online trace logging, collect run-time
information for the targeted program under the dynamic binary instrumentation (DBI)
to collect necessary information. The second step, called offline analysis, run
several analysis (e.g., taint analysis, symbolic analysis) on the top of traces.  

Existing binary analysis frameworks, like Angr, BAP and Bitblaze combine both static
and dynamic analyses, are applicable to a series of tasks. However, those frameworks
are not suitable for the following reasons. 
\begin{itemize}
    \item Existing binary analysis frameworks transfer the machine code into
    architecture-independent intermediate representatives (IR) and then run the
    analysis. The IR design, which simplify the implementation and support multiple
    architectures, introduces imprecision to the analysis results as well because some
    information like cache accesses, control-flow transfers are lost during the IR
    transformation.
    \item Coverage is one of the concerns when they design the frameworks. 
    As a result, they rely on constraint solvers to find different inputs that can
    trigger different paths. As constraint solving is known for time-consuming. It is
    one of the obstacles to make analyses salable.
    However, we will show for many tasks, like binary similarity comparisons and side-channel vulnerability detection. We can
    reduce the call to the solver just by fuzzing the formula. The design will
    significantly reduce the overhead.
\end{itemize}
To solve the above problem, we propose Phoenix, a trace-oriented binary analysis
framework. Phoenix is a dynamic binary analysis framework that provide several 
functioning including dynamic symbolic execution engine, dynamic taint analysis
engine, AST representation of X86 and binding for STP solvers. It run 
execution on the top X86 instructions and symbolically reason the program behaviours.
Our preliminary results show that Phoenix has a better performance compared to 
many existing works, which makes it scalable to real-world applications.




\section{Dissertation Organization}
The rest of the dissertation is organized as follows. We first present relevant backgrounds and related works in Chapter~\ref{chapter1}. We then propose our works in
Chapter~\ref{chapter2}. Chapter~\ref{chapter3} and Chapter~\ref{chapter4} introduce
our recent progress about the proposed work. 
